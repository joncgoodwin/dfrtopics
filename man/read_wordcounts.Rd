% Generated by roxygen2 (4.1.1.9000): do not edit by hand
% Please edit documentation in R/dfr_docs.R
\name{read_wordcounts}
\alias{read_wordcounts}
\title{Convert DfR wordcount files to a long-format data frame}
\usage{
read_wordcounts(files, filename_id = dfr_filename_id)
}
\arguments{
\item{files}{individual filenames to read.}

\item{filename_id}{function that converts a file name into a document ID. By
default, \code{\link{dfr_filename_id}} is used.}
}
\value{
A data frame with three columns: \code{id}, the document ID;
  \code{word}, a word type or term (called \code{WORDCOUNTS} in DfR source
  data files); \code{weight}, the count.
}
\description{
Reads in a bunch of \code{wordcounts*.CSV} files and stacks them up in a
single long-format dataframe. These counts can be optionally manipulated,
then passed on to \code{\link{wordcounts_texts}} then
\code{\link{make_instances}}.
}
\details{
Empty documents are skipped; DfR supplies wordcounts files for documents that
have no wordcount data. These will be in DfR's metadata but not in the output
dataframe here.

This is slow. An outboard script in python or Perl is faster, but this keeps
us in R and does everything in memory.

Memory usage: for N typical journal articles, the resulting dataframe seems
to need about 20N K of memory. So R on a laptop will hit its limits somewhere
around 100K articles of typical length.
}
\seealso{
\code{\link{wordcounts_texts}}, \code{\link{instances_Matrix}} for
  word counts \emph{after} stopword removal (etc.).
}

